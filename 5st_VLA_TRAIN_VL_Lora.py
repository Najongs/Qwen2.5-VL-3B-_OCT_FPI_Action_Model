import argparse
import wandb
import io, shutil, threading, queue, time
import os
import re
import math
import glob
import pickle
import numpy as np
import torch
from pathlib import Path
from tqdm import tqdm

import torch
import torch.nn as nn

from torch.utils.data import Dataset, ConcatDataset, DataLoader
from torch.utils.data import random_split

import torch.distributed as dist
from torch.utils.data import DataLoader, DistributedSampler
from qwen_vl_utils import process_vision_info
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor

from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim.lr_scheduler import LambdaLR

import random
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(True, warn_only=True)
torch.set_float32_matmul_precision("high")

from model import QwenActionExpert, Not_freeze_QwenVLAForAction
from Total_Dataset import collate_fn, infer_lang_from_path
from Total_Dataset import BridgeRawSequenceDataset, insertionMeca500Dataset
from Make_VL_cache import build_vl_cache_distributed_optimized

# ======== I/O & Checkpoint Utils ========
STAGING_DIR = Path("/dev/shm/qwen_vla_stage")   # ë¡œì»¬ RAM/NVMe (ì—†ìœ¼ë©´ /tmp ê¶Œì¥)
CKPT_DIR     = Path("./checkpoints")            # NAS ë˜ëŠ” ê³µìœ  ë””ë ‰í† ë¦¬
STAGING_DIR.mkdir(parents=True, exist_ok=True)
CKPT_DIR.mkdir(parents=True, exist_ok=True)

def _atomic_move(src: Path, dst: Path):
    dst.parent.mkdir(parents=True, exist_ok=True)
    tmp = dst.with_suffix(dst.suffix + ".tmp")
    if src != tmp:
        shutil.copy2(src, tmp)
    os.replace(tmp, dst)

def copy_to_local_then_load(src_path: Path, map_location):
    """ë„¤íŠ¸ì›Œí¬ íŒŒì¼ì„ ë¡œì»¬ ìŠ¤í…Œì´ì§•ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³µì‚¬ í›„ torch.load"""
    if not src_path.exists():
        raise FileNotFoundError(str(src_path))
    local_copy = STAGING_DIR / src_path.name
    shutil.copy2(src_path, local_copy)  # ë³´í†µ ì´ ê²½ë¡œê°€ í›¨ì”¬ ë¹ ë¦„
    # PyTorch 2.4+ë©´ weights_only=Trueê°€ ë¹ ë¥´ê³  ì•ˆì „
    try:
        return torch.load(local_copy, map_location=map_location, weights_only=False)
    except TypeError:
        return torch.load(local_copy, map_location=map_location)

class AsyncCheckpointWriter:
    """í•™ìŠµì€ ê·¸ëŒ€ë¡œ ì§„í–‰, ì €ì¥ì€ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œê°€ ì²˜ë¦¬"""
    def __init__(self, max_queue=2, sync_every=0):
        self.q = queue.Queue(maxsize=max_queue)
        self.thread = threading.Thread(target=self._worker, daemon=True)
        self.stop = False
        self.sync_every = sync_every  # 0ì´ë©´ ì¦‰ì‹œ ì²˜ë¦¬
        self.thread.start()

    def _worker(self):
        last_sync = time.time()
        while not self.stop:
            try:
                payload = self.q.get(timeout=0.5)
            except queue.Empty:
                continue
            state_dict, final_dst = payload["state"], Path(payload["dst"])
            # 1) ë¡œì»¬ ìŠ¤í…Œì´ì§•ì— ë¨¼ì € ì €ì¥ (CPU í…ì„œ)
            local_tmp = STAGING_DIR / (final_dst.name + f".{int(time.time())}.pt")
            # ëª¨ë¸/ì˜µí‹°ë§ˆì´ì € í…ì„œë¥¼ CPUë¡œ ë³µì‚¬í•œ ìƒíƒœë¥¼ ë°›ëŠ” ê²ƒì´ ì´ìƒì 
            torch.save(state_dict, local_tmp, _use_new_zipfile_serialization=True)
            # 2) í•„ìš” ì‹œ ë°°ì¹˜/ì£¼ê¸° ë™ê¸°í™”
            if self.sync_every > 0 and (time.time() - last_sync) < self.sync_every:
                continue
            # 3) ì›ìì  êµì²´ë¡œ ìµœì¢… ëª©ì ì§€ì— ë°˜ì˜
            _atomic_move(local_tmp, final_dst)
            last_sync = time.time()

    def submit(self, state_dict, final_dst: Path):
        # íê°€ ê°€ë“ ì°¨ ìˆìœ¼ë©´ ê°€ì¥ ì˜¤ë˜ëœ ê±¸ ë²„ë¦¬ê³  ìµœì‹ ìœ¼ë¡œ êµì²´(í•™ìŠµ ì§€ì—° ë°©ì§€)
        if self.q.full():
            try:
                self.q.get_nowait()
            except queue.Empty:
                pass
        self.q.put({"state": state_dict, "dst": str(final_dst)})

    def close(self):
        self.stop = True
        self.thread.join(timeout=5)

def build_trapezoid_scheduler(
    optimizer,
    total_steps: int,
    *,
    base_lr: float = 1e-4,
    min_lr: float = 1e-6,
    warmup_ratio: float = 0.03,
    hold_ratio: float = 0.02,
):
    """
    LLM ìŠ¤íƒ€ì¼: Warmup -> Hold(ì„ íƒ) -> Cosine Decay
    - warmup_ratio: ì „ì²´ step ëŒ€ë¹„ ì›Œë°ì—… ë¹„ìœ¨
    - hold_ratio  : ì›Œë°ì—… í›„ ê³ ì • ìœ ì§€ ë¹„ìœ¨ (0 ê°€ëŠ¥)
    - ë‚˜ë¨¸ì§€ëŠ” cosineìœ¼ë¡œ min_lrê¹Œì§€ ê°ì‡ 
    """
    warmup_steps = int(total_steps * warmup_ratio)
    hold_steps   = int(total_steps * hold_ratio)
    decay_steps  = max(1, total_steps - warmup_steps - hold_steps)
    floor = min_lr / max(base_lr, 1e-12)

    def lr_lambda(step: int):
        if step < warmup_steps:
            # ì„ í˜• ì›Œë°ì—…: 0 -> 1
            return (step + 1) / max(1, warmup_steps)
        elif step < warmup_steps + hold_steps:
            # ìœ ì§€: 1.0
            return 1.0
        else:
            # ì½”ì‚¬ì¸ ê°ì‡ : 1 -> floor
            t = (step - warmup_steps - hold_steps) / decay_steps
            t = min(max(t, 0.0), 1.0)
            cos = 0.5 * (1 + math.cos(math.pi * t))  # 1 -> 0
            return floor + (1 - floor) * cos

    return LambdaLR(optimizer, lr_lambda=lr_lambda)

def build_rewarm_scheduler(
    optimizer,
    total_steps: int,
    *,
    prev_lr: float,        # ë§ˆì§€ë§‰ í•™ìŠµì—ì„œ ì“°ë˜ ì‹¤ lr (ì˜ˆ: 1e-8)
    target_lr: float = 1e-4,
    min_lr: float = 1e-6,
    warmup_ratio: float = 0.05,
    hold_ratio: float = 0.05,
):
    """
    ğŸ” ReWarm Scheduler (ì ˆëŒ€ lr ìŠ¤ì¼€ì¤„ì„ factorë¡œ í‘œí˜„)
      - base_lr = target_lr ë¡œ ì¡ê³ 
      - factorë¥¼ prev_lr/target_lr â†’ 1.0 ìœ¼ë¡œ warmup
      - hold í›„ 1.0 â†’ (min_lr/target_lr) ë¡œ cosine decay
    """
    assert target_lr > 0 and min_lr > 0
    warmup_steps = int(total_steps * warmup_ratio)
    hold_steps   = int(total_steps * hold_ratio)
    decay_steps  = max(1, total_steps - warmup_steps - hold_steps)

    floor = min_lr / target_lr                 # decay ìµœí•˜ í•œê³„ (factor)
    start = max(1e-12, prev_lr / target_lr)    # warmup ì‹œì‘ factor(ì•„ì£¼ ì‘ì„ ìˆ˜ ìˆìŒ)

    def lr_lambda(step: int):
        if step < warmup_steps:
            # ì„ í˜•: start â†’ 1.0
            prog = (step + 1) / max(1, warmup_steps)
            return start + (1.0 - start) * prog
        elif step < warmup_steps + hold_steps:
            return 1.0
        else:
            # cosine: 1.0 â†’ floor
            t = (step - warmup_steps - hold_steps) / decay_steps
            t = min(max(t, 0.0), 1.0)
            cos = 0.5 * (1 + math.cos(math.pi * t))   # 1 â†’ 0
            return floor + (1.0 - floor) * cos

    # â‘  base_lrë¥¼ target_lrë¡œ ë§ì¶¤
    for g in optimizer.param_groups:
        g["lr"] = target_lr

    sched = LambdaLR(optimizer, lr_lambda=lr_lambda)

    # â‘¡ ì²« step ì „ì— ì‹¤ lrì„ prev_lrë¡œ ë§ì¶° ì‹œì‘ (warmup ì‹œì‘ì )
    for g in optimizer.param_groups:
        g["lr"] = prev_lr
    return sched

# ===========================================================
# 1ï¸âƒ£ ì´ˆê¸°í™”
# ===========================================================
def setup_distributed():
    dist.init_process_group(backend="nccl")
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    return rank, world_size, local_rank

# ===========================================================
# 2ï¸âƒ£ í•™ìŠµ ë£¨í”„
# ===========================================================
def Train(
    model,
    data_loader,
    optimizer,
    num_epochs=3,
    grad_accum_steps=8,
    device="cuda",
    save_path="./checkpoints/qwen_vla.pt",
    scheduler=None,
    sched_on="step",
    val_loader=None,
    start_epoch=0,           # âœ… ì¶”ê°€
):
    loss_fn = nn.MSELoss()
    rank = dist.get_rank()
    writer = AsyncCheckpointWriter(max_queue=2, sync_every=0) if rank == 0 else None

    model.train()
    if rank == 0:
        wandb.init(
            project="QwenVLA",
            name=f"train_run_{time.strftime('%m%d_%H%M')}",
            resume="allow",
            id=f"qvla_{int(time.time())}",
            settings=wandb.Settings(start_method="thread", _disable_stats=True),
            config={
                "lr": optimizer.param_groups[0]["lr"],
                "grad_accum_steps": grad_accum_steps,
                "epochs": num_epochs,
                "scheduler": sched_on,
            }
        )

    global_step = 0
    for epoch in range(start_epoch, start_epoch + num_epochs):
        if isinstance(data_loader.sampler, DistributedSampler):
            data_loader.sampler.set_epoch(epoch)

        total_loss = 0.0
        optimizer.zero_grad()
        model.train()

        pbar = tqdm(enumerate(data_loader), total=len(data_loader),
                    desc=f"[Rank {rank}] Epoch {epoch+1}",
                    disable=(rank != 0))

        for step, batch in pbar:
            instructions = batch["instruction"]
            image_inputs = batch["images"]
            gt_actions = batch["actions"].to(device, dtype=torch.bfloat16)

            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                pred_actions, _ = model(
                    text_inputs=instructions,
                    image_inputs=image_inputs,
                    z_chunk=gt_actions,
                    cache_keys=batch["cache_keys"],
                )

                weights = torch.tensor(batch["confidence"], device=device, dtype=torch.bfloat16)
                weights = weights / weights.mean()
                loss_each = (pred_actions.float() - gt_actions.float()).pow(2).mean(dim=[1,2])
                loss = (loss_each * weights).mean() / grad_accum_steps

            loss.backward()
            total_loss += loss.item() * grad_accum_steps

            # === gradient step ===
            if (step + 1) % grad_accum_steps == 0:
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                if scheduler is not None and sched_on == "step":
                    scheduler.step()

                global_step += 1

                # === âœ… tqdm ë° wandb ë¡œê¹… ===
                lr = optimizer.param_groups[0]["lr"]
                if rank == 0:
                    pbar.set_postfix({
                        "loss": f"{loss.item() * grad_accum_steps:.6f}",
                        "lr": f"{lr:.2e}",
                        "grad": f"{grad_norm:.2f}"
                    })
                    wandb.log({
                        "train/loss_step": loss.item() * grad_accum_steps,
                        "train/lr": lr,
                        "train/grad_norm": grad_norm,
                        "global_step": global_step
                    })

        # === epoch í‰ê·  ===
        avg_loss_tensor = torch.tensor(total_loss / len(data_loader), device=device)
        dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.AVG)
        avg_loss = avg_loss_tensor.item()

        # === scheduler per epoch ===
        if scheduler is not None and sched_on == "epoch":
            scheduler.step()

        # === âœ… Validation Loop ===
        val_loss = None
        if val_loader is not None:
            model.eval()
            val_loss_sum, val_count = 0.0, 0
            with torch.no_grad():
                for batch in val_loader:
                    gt_actions = batch["actions"].to(device, dtype=torch.bfloat16)
                    pred_actions, _ = model(
                        text_inputs=batch["instruction"],
                        image_inputs=batch["images"],
                        z_chunk=gt_actions,
                        cache_keys=batch["cache_keys"],
                    )
                    weights = torch.tensor(batch["confidence"], device=device, dtype=torch.bfloat16)
                    weights = weights / weights.mean()  # í‰ê·  1ë¡œ ì •ê·œí™”
                    loss_each = (pred_actions.float() - gt_actions.float()).pow(2).mean(dim=[1,2])  # ìƒ˜í”Œë³„ MSE
                    loss = (loss_each * weights).mean() / grad_accum_steps
                    val_loss_sum += loss.item()
                    val_count += 1
            val_loss = val_loss_sum / max(1, val_count)
            model.train()


    
        # === epoch ì¢…ë£Œ í›„ ===
        if rank == 0:
            trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in model.parameters())
            frozen = total_params - trainable

            import psutil, gc
            gpu_mem = torch.cuda.memory_allocated()/1e9
            cpu_mem = psutil.virtual_memory().percent
            gc.collect()

            # === ì¶”ê°€ ë¡œê¹… ===
            wandb.log({
                "epoch": epoch + 1,
                "train/loss_epoch": avg_loss,
                "val/loss_epoch": val_loss if val_loss else None,
                "params/trainable_M": trainable / 1e6,
                "params/frozen_M": frozen / 1e6,
                "params/frozen_ratio": frozen / total_params,
                "system/gpu_mem_GB": gpu_mem,
                "system/cpu_mem_%": cpu_mem,
                "lr/base_lr": optimizer.param_groups[0]["lr"],
                "lr/vl_lr": optimizer.param_groups[1]["lr"] if len(optimizer.param_groups) > 1 else None,
                "lr/vision_lr": optimizer.param_groups[2]["lr"] if len(optimizer.param_groups) > 2 else None,
                "scheduler/phase_ratio_warmup": scheduler._get_lr_lambda(0) if hasattr(scheduler, "_get_lr_lambda") else None,
            })

            print(f"[DEBUG] GPU {gpu_mem:.2f} GB / CPU {cpu_mem:.1f}% used "
                  f"| Trainable {trainable/1e6:.2f}M / Frozen {frozen/1e6:.2f}M")

            # === LoRA íŒŒë¼ë¯¸í„° ë¡œê¹… (ì„ íƒ) ===
            lora_params = {n: p for n, p in model.named_parameters() if "lora_" in n}
            if lora_params:
                avg_abs = np.mean([p.data.abs().mean().item() for p in lora_params.values()])
                wandb.log({"lora/avg_weight_abs": avg_abs})

            print(f"\nğŸ“Š Epoch {epoch+1} Summary | Train: {avg_loss:.8f} | "
                  f"Val: {val_loss:.8f}" if val_loss else f"\nğŸ“Š Epoch {epoch+1} Train Loss: {avg_loss:.8f}")

            save_dir = Path(save_path).parent
            save_dir.mkdir(parents=True, exist_ok=True)
            base_path = Path(save_path)

            # === Best model ì €ì¥ ì—¬ë¶€ íŒë‹¨ ===
            if not hasattr(Train, "_best_loss"):
                Train._best_loss = float("inf")

            is_best = val_loss is not None and val_loss < Train._best_loss

            if is_best:
                Train._best_loss = val_loss
                best_path = save_dir / "qwen_vla_best.pt"
                torch.save({
                    "epoch": epoch,
                    "model_state_dict": model.module.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
                    "val_loss": val_loss,
                }, best_path)
                print(f"ğŸ† [Best] Validation improved â†’ saved to {best_path}")

            else:
                # Bestê°€ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ latestë§Œ ë®ì–´ì“°ê¸°
                tmp_path = base_path.with_suffix(".tmp")
                torch.save({
                    "epoch": epoch,
                    "model_state_dict": model.module.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
                    "val_loss": val_loss,
                }, tmp_path)
                os.replace(tmp_path, base_path)
                print(f"ğŸ’¾ [Sync] Latest checkpoint updated: {base_path}")

    if rank == 0 and writer is not None:
        writer.close()

    if rank == 0:
        wandb.finish()

def main():
    parser = argparse.ArgumentParser()
    # ===== ê¸°ì¡´ VLA ê¸°ë³¸ í•™ìŠµ ì˜µì…˜ =====
    parser.add_argument("--mode", choices=["cache", "train"], required=True,
                        help="Mode: 'cache' to build feature cache, 'train' to train action expert")
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--min-lr", type=float, default=1e-6)
    parser.add_argument("--warmup-ratio", type=float, default=0.03)
    parser.add_argument("--hold-ratio", type=float, default=0.02)
    parser.add_argument("--grad-accum-steps", type=int, default=8)
    parser.add_argument("--sched-on", choices=["step", "epoch"], default="step")

    # ===== ì¶”ê°€ëœ LoRA / Fine-tuning ì˜µì…˜ =====
    parser.add_argument("--finetune-vl", choices=["none", "lora", "full"], default="lora",
                        help="Qwen-VL fine-tuning mode")
    parser.add_argument("--vl-lr", type=float, default=1e-5,
                        help="learning rate for VL backbone (LoRA/full mode)")
    parser.add_argument("--vision-lr", type=float, default=5e-6,
                        help="learning rate for vision encoder if full fine-tuning")
    parser.add_argument("--lora-r", type=int, default=16)
    parser.add_argument("--lora-alpha", type=int, default=32)
    parser.add_argument("--lora-dropout", type=float, default=0.05)
    parser.add_argument("--unfreeze-last-n", type=int, default=2,
                        help="number of transformer blocks to unfreeze when full fine-tuning")

    args = parser.parse_args()

    rank, world_size, local_rank = setup_distributed()
    
    device = torch.device(f"cuda:{local_rank}")

    if rank == 0:
        print(f"ğŸš€ [Rank {rank}] Running in {args.mode.upper()} mode on {world_size} GPUs")

    # # === 1ï¸âƒ£ BRIDGE ë°ì´í„°ì…‹ ===
    # bridge_root = "/home/najo/NAS/VLA/dataset/raw"
    # bridge_ds = BridgeRawSequenceDataset(root=bridge_root, horizon=8)

    # === 2ï¸âƒ£ Meca500 (OCT Insertion) ===
    json_file_path_list = [
        f"/home/najo/NAS/VLA/dataset/OCT_insertion/Captures{i}/Captures{i}_precise_9views.json"
        for i in range(1, 8)
    ]
    meca_datasets = [insertionMeca500Dataset(json_path=p, horizon=8) for p in json_file_path_list]
    meca_ds = ConcatDataset(meca_datasets)

    # === 3ï¸âƒ£ ZED ê¸°ë°˜ ì¶”ê°€ íŒŒíŠ¸ ===
    zed_paths = []
    for i in range(1, 21):
        part = "part1" if i <= 10 else "part2"
        zed_paths.append(f"/home/najo/NAS/VLA/dataset/{part}/ZED_Captures_{i}th/ZED_Captures_{i}th_precise_8views.json")

    zed_datasets = [insertionMeca500Dataset(json_path=p, horizon=8) for p in zed_paths]
    zed_ds = ConcatDataset(zed_datasets)

    # === 4ï¸âƒ£ ì „ì²´ í†µí•© ===
    dataset = ConcatDataset([meca_ds, zed_ds]) # bridge_ds, 

    if rank == 0:
        print(f"âœ… Unified dataset size: {len(dataset)} samples.")

    
    if rank == 0:
        print(f"âœ… [Rank {rank}] Dataset initialized with {len(dataset)} samples.")

    vl_model_name = "Qwen/Qwen2.5-VL-3B-Instruct"

    # ===========================================================
    # 3ï¸âƒ£ ìºì‹œ ìƒì„± ëª¨ë“œ
    # ===========================================================
    if args.mode == "cache":
        if rank == 0:
            print("â³ Initializing VL-only model for cache building...")

        processor = AutoProcessor.from_pretrained(vl_model_name)
        vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            vl_model_name,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
            device_map="cuda",
            low_cpu_mem_usage=True,          # â† ë¡œë”© ì†ë„/ë©”ëª¨ë¦¬ ìµœì í™”
        )

        class DummyVLA:
            def __init__(self, vl_model, processor):
                self.vl_model = vl_model
                self.processor = processor
                self.cache_dir = Path("/home/najo/NAS/VLA/dataset/cache/qwen_vl_features")
                self.cache_dir.mkdir(parents=True, exist_ok=True)

                # âœ… ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œë§Œ ë°”ì¸ë”©
                self._cache_path = Not_freeze_QwenVLAForAction._cache_path.__get__(self)
                self._enforce_cache_limit = Not_freeze_QwenVLAForAction._enforce_cache_limit.__get__(self)

                # âœ… staticmethodëŠ” ê·¸ëŒ€ë¡œ ë³µì‚¬
                self._atomic_save = Not_freeze_QwenVLAForAction._atomic_save

            def eval(self):
                self.vl_model.eval()
                return self

        dummy_model = DummyVLA(vl_model, processor)

        # ìºì‹œ ë¹Œë“œ
        build_vl_cache_distributed_optimized(
            dummy_model, dataset, device=device,
            rank_sharded_cache=False
        )
        
        dist.barrier()
        if rank == 0:
            print("âœ… Cache build complete. You can now run training with --mode train.")
        dist.destroy_process_group()
        return  # ìºì‹œ í›„ ì¢…ë£Œ

    # ===========================================================
    # 4ï¸âƒ£ í•™ìŠµ ëª¨ë“œ
    # ===========================================================
    if args.mode == "train":
        if rank == 0:
            print("â³ Initializing full QwenVLA model for training...")

        model = Not_freeze_QwenVLAForAction(
            vl_model_name=vl_model_name,
            action_dim=7,
            horizon=8,
            hidden_dim=1024,
            finetune_vl=args.finetune_vl,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            unfreeze_last_n=args.unfreeze_last_n
        ).to(device)
        
        # ì „ì²´ ë°ì´í„°ì…‹ ë¶„í• 
        total_len = len(dataset)
        val_len = int(total_len * 0.05)   # 5% validation
        train_len = total_len - val_len
        train_ds, val_ds = random_split(dataset, [train_len, val_len])

        # DDPìš© Sampler
        train_sampler = DistributedSampler(train_ds, num_replicas=world_size, rank=rank, shuffle=True)
        val_sampler   = DistributedSampler(val_ds,   num_replicas=world_size, rank=rank, shuffle=False)

        # ê°ê°ì— ëŒ€í•œ DataLoader
        train_loader = DataLoader(
            train_ds,
            batch_size=1,
            num_workers=4,
            sampler=train_sampler,
            collate_fn=collate_fn,
            prefetch_factor=4,
            persistent_workers=False,
            pin_memory=False
        )
        val_loader = DataLoader(
            val_ds,
            batch_size=1,
            num_workers=4,
            sampler=val_sampler,
            collate_fn=collate_fn,
            persistent_workers=False,
            pin_memory=False,
        )

        model = DDP(model, device_ids=[local_rank], find_unused_parameters=True)
        
        # === Optimizer êµ¬ì„± ===
        def wd_filter(name, param):
            if param.ndim == 1: return False
            if name.endswith(".bias"): return False
            return True

        ae_named = list(model.module.action_expert.named_parameters())
        vl_named = list(model.module.vl_model.named_parameters())

        ae_decay    = [p for n,p in ae_named if wd_filter(n,p) and p.requires_grad]
        ae_n_decay  = [p for n,p in ae_named if not wd_filter(n,p) and p.requires_grad]

        vl_decay   = [p for n,p in vl_named if wd_filter(n,p) and p.requires_grad]
        vl_n_decay = [p for n,p in vl_named if not wd_filter(n,p) and p.requires_grad]

        vision_decay, vision_n_decay = [], []
        for n,p in vl_named:
            if not p.requires_grad:
                continue
            if "vision" in n or "visual" in n or "vision_tower" in n:
                (vision_decay if wd_filter(n,p) else vision_n_decay).append(p)

        # === LR ì„¤ì • ===
        param_groups = [
            {"params": ae_decay,        "lr": args.lr,      "weight_decay": 0.01},
            {"params": ae_n_decay,      "lr": args.lr,      "weight_decay": 0.0},
        ]

        if args.finetune_vl == "lora":
            param_groups += [
                {"params": vl_decay,    "lr": args.vl_lr,   "weight_decay": 0.01},
                {"params": vl_n_decay,  "lr": args.vl_lr,   "weight_decay": 0.0},
            ]
        elif args.finetune_vl == "full":
            param_groups += [
                {"params": vision_decay,    "lr": args.vision_lr,  "weight_decay": 0.01},
                {"params": vision_n_decay,  "lr": args.vision_lr,  "weight_decay": 0.0},
                {"params": vl_decay,        "lr": args.vl_lr,      "weight_decay": 0.01},
                {"params": vl_n_decay,      "lr": args.vl_lr,      "weight_decay": 0.0},
            ]

        optimizer = torch.optim.AdamW(param_groups, betas=(0.9, 0.999), eps=1e-8)

        
        # =======================================================
        # âœ… ì²´í¬í¬ì¸íŠ¸ ë³µì› (model+optim) + epoch ë³µì›
        # =======================================================
        ckpt_path = "./checkpoints/qwen_vla_final_1000.pt"
        start_epoch = 0
        if os.path.exists(ckpt_path):
            if rank == 0:
                print(f"ğŸ”„ Found checkpoint at {ckpt_path}, resuming training...")
            checkpoint = copy_to_local_then_load(Path(ckpt_path), map_location=device)

            try:
                model.module.load_state_dict(checkpoint["model_state_dict"], strict=False)
                print("âœ… Loaded model weights (partial, strict=False)")
            except KeyError:
                model.module.load_state_dict(checkpoint, strict=False)


            if "optimizer_state_dict" in checkpoint:
                try:
                    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
                except ValueError:
                    print("âš ï¸ Optimizer group mismatch detected â€” skipping optimizer state load.")


            # âœ… ì—¬ê¸°ì„œ epoch ë°˜ë“œì‹œ ë³µì›
            if "epoch" in checkpoint:
                start_epoch = checkpoint["epoch"] + 1
                if rank == 0:
                    print(f"âœ… Resumed from epoch {start_epoch}")
        else:
            if rank == 0:
                print("ğŸ†• No checkpoint found, starting from scratch.")

        # ===== ë‚¨ì€ epoch/step ê³„ì‚° =====
        total_epochs = 1100
        remaining_epochs = max(1, total_epochs - start_epoch)
        iters_per_epoch = len(train_loader)
        steps_per_epoch = math.ceil(iters_per_epoch / max(1, args.grad_accum_steps))
        total_steps = steps_per_epoch * remaining_epochs

        # ===== ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • =====
        current_lr = optimizer.param_groups[0]["lr"]   # ckptì—ì„œ ë¶ˆëŸ¬ì˜¨ í˜„ì¬ lr (ex. 1e-8)
        target_lr  = args.lr

        if start_epoch > 0:
            # ì´ì–´ ë‹¬ë¦¬ê¸° â†’ prev_lr(=current_lr)ì—ì„œ target_lrë¡œ rewarm
            scheduler = build_rewarm_scheduler(
                optimizer,
                total_steps=total_steps,
                prev_lr=current_lr,
                target_lr=target_lr,
                min_lr=args.min_lr,
                warmup_ratio=0.05,
                hold_ratio=0.05,
            )
            if rank == 0:
                print(f"ğŸ” [ReWarm] {current_lr:.2e} â†’ {target_lr:.2e}, then cosine â†’ {args.min_lr:.2e}")
        else:
            # ì²˜ìŒ ì‹œì‘ â†’ ì¼ë°˜ trapezoid
            scheduler = build_trapezoid_scheduler(
                optimizer,
                total_steps=total_steps,
                base_lr=target_lr,
                min_lr=args.min_lr,
                warmup_ratio=args.warmup_ratio,
                hold_ratio=args.hold_ratio,
            )
            if rank == 0:
                print(f"ğŸ†• [New] 0 â†’ {target_lr:.2e} (warmup/hold/cosine)")

        save_path = './checkpoints/qwen_vla_train.pt'
        # ==================================F=====================
        # âœ… Train í˜¸ì¶œ (ë‚¨ì€ epochë§Œí¼ ì§„í–‰)
        # =======================================================
        Train(
            model,
            train_loader,
            optimizer,
            num_epochs=remaining_epochs,
            grad_accum_steps=args.grad_accum_steps,
            device=device,
            save_path=save_path,
            scheduler=scheduler,
            sched_on=args.sched_on,
            val_loader=val_loader,
            start_epoch=start_epoch
        )

        # =======================================================
        # âœ… ì•ˆì „ ì €ì¥ ë³´ì¥ (Async writer flush)
        # =======================================================
        if rank == 0 and "writer" in locals() and writer is not None:
            print("ğŸ’¾ Flushing async checkpoints to disk...")
            writer.flush()
            writer.close()
            print("âœ… All pending checkpoints flushed safely.")

        # âœ… ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ëª¨ë“  ìƒíƒœ í¬í•¨)
        if rank == 0:
            final_path = Path("./checkpoints/qwen_vla_final.pt")
            torch.save({
                "epoch": total_epochs - 1,
                "model_state_dict": model.module.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
            }, final_path)
            print(f"âœ… [Final] Final checkpoint saved at {final_path}")

        dist.destroy_process_group()
        if rank == 0:
            print("ğŸ§¹ DDP process group destroyed. Training complete.")

if __name__ == "__main__":
    os.environ["PYTHONBUFFERED"] = "1"
    torch.multiprocessing.set_start_method("spawn", force=True)
    main()
