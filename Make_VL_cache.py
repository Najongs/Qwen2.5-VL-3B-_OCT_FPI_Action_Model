
import os
import math

from pathlib import Path
import hashlib, fcntl

from tqdm import tqdm

import torch
import torch.distributed as dist

from qwen_vl_utils import process_vision_info
from torch.utils.data import DataLoader, DistributedSampler

from Total_Dataset import collate_fn

# =====================================
# 1ï¸âƒ£ Action Expert (Temporal Decoder)
# =====================================
def build_vl_cache_distributed_optimized(
    model,
    dataset,
    device="cuda",
    *,
    batch_size=512,          # DataLoader ë°°ì¹˜ (VRAM 24GBë©´ 2~4 ê¶Œì¥)
    num_workers=8,
    prefetch_factor=4,
    micro_bs=2,            # ë§ˆì´í¬ë¡œ ë°°ì¹˜ (OOM ì‹œ ìë™ ë°±ì˜¤í”„)
    key_mode="full",       # "traj": traj_path ë‹¨ìœ„ ìºì‹œ / "full": (traj+lang+views) ë‹¨ìœ„
    rank_sharded_cache=False,  # rankë³„ ìºì‹œ í´ë” ë¶„ë¦¬
    cache_dir_fallback="/home/najo/NAS/VLA/dataset/cache/qwen_vl_features",
):
    """
    ê³ ì†/ì•ˆì • ìºì‹±:
      - ë§ˆì´í¬ë¡œë°°ì¹­ + OOM ë°±ì˜¤í”„
      - use_cache=False (KV cache ë¹„í™œì„±í™”)
      - rankë³„ ìºì‹œ í´ë” ë¶„ë¦¬ë¡œ fcntl ë½ ê²½í•© ìµœì†Œí™”
      - tqdm ì§„í–‰ë¥ , miss/skipped í†µê³„ í‘œì‹œ
      - key_mode="traj"ë¡œ ë‘ë©´ ë™ì¼ trajëŠ” 1íšŒë§Œ ê³„ì‚° (ì†ë„ 10~30ë°°â†‘)

    model ìš”êµ¬ì‚¬í•­:
      - model.vl_model, model.processor í•„ìš”
      - (ì„ íƒ) model.cache_dir ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ cache_dir_fallback ì‚¬ìš©
      - (ì„ íƒ) model._atomic_save / model._enforce_cache_limit ìˆìœ¼ë©´ ì‚¬ìš©
    """

    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # base cache dir
    base_cache_dir = getattr(model, "cache_dir", None)
    if base_cache_dir is None:
        base_cache_dir = Path(cache_dir_fallback)
    else:
        base_cache_dir = Path(base_cache_dir)

    # rank-sharded dir (ë½ ê²½ìŸ ìµœì†Œí™”)
    target_cache_dir = base_cache_dir / (f"rank_{rank}" if rank_sharded_cache else "")
    target_cache_dir.mkdir(parents=True, exist_ok=True)

    # ---------------------------
    # ë¡œì»¬ í—¬í¼: ì•ˆì „ ì €ì¥/ê²½ë¡œ ìƒì„±
    # ---------------------------
    def _local_atomic_save(tensor_cpu: torch.Tensor, path: Path):
        """modelì— _atomic_saveê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¡œì»¬ êµ¬í˜„"""
        if hasattr(model, "_atomic_save"):
            return model._atomic_save(tensor_cpu, path)
        tmp = path.with_suffix(path.suffix + ".tmp")
        lock_path = str(path) + ".lock"
        with open(lock_path, "w") as lockfile:
            try:
                fcntl.flock(lockfile, fcntl.LOCK_EX)
                if path.exists():
                    return
                torch.save(tensor_cpu, tmp)
                os.replace(tmp, path)
            finally:
                fcntl.flock(lockfile, fcntl.LOCK_UN)

    def _local_enforce_cache_limit(max_gb=40):
        """modelì— _enforce_cache_limit ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¡œì»¬ë¡œ target_cache_dirë§Œ ì •ë¦¬"""
        if hasattr(model, "_enforce_cache_limit"):
            try:
                # modelì´ ìê¸° cache_dirì„ ê´€ë¦¬í•œë‹¤ë©´ í˜¸ì¶œ
                model._enforce_cache_limit(max_gb=max_gb)
                return
            except TypeError:
                pass  # ì‹œê·¸ë‹ˆì²˜ ì°¨ì´ ë¬´ì‹œí•˜ê³  ë¡œì»¬ ì²˜ë¦¬ë¡œ í´ë°±
        total_bytes = 0
        files = []
        for f in target_cache_dir.glob("*.pt"):
            total_bytes += f.stat().st_size
            files.append(f)
        limit = max_gb * (1024 ** 3)
        if total_bytes > limit:
            files = sorted(files, key=lambda f: f.stat().st_mtime)
            while total_bytes > limit and files:
                f = files.pop(0)
                total_bytes -= f.stat().st_size
                f.unlink(missing_ok=True)

    def _cache_path_for(traj_key: str, txt: str, views: list[str | None]) -> Path:
        """key_modeì— ë”°ë¼ ìºì‹œ í‚¤ êµ¬ì„±."""
        if key_mode == "traj":
            # traj_pathë§Œ ê¸°ì¤€ (ê°™ì€ trajëŠ” 1íšŒë§Œ ê³„ì‚°)
            base = traj_key.split("::t=")[0]
        else:
            # ê¸°ì¡´ ë°©ì‹: traj+lang+views ëª¨ë‘ ë°˜ì˜
            vlist = [v for v in views if v is not None]
            base = traj_key + "||" + txt + "||" + "|".join(vlist)
        h = hashlib.sha1(base.encode("utf-8")).hexdigest()[:24]
        return target_cache_dir / f"{h}.pt"

    # ---------------------------
    # DataLoader (ìƒ˜í”Œ ë¶„ë°° ë³´ì¥)
    # ---------------------------
    sampler = DistributedSampler(
        dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False
    )
    data_loader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        sampler=sampler,
        collate_fn=collate_fn,
        prefetch_factor=prefetch_factor,
        pin_memory=True,
        persistent_workers=True,
    )

    total_local = math.ceil(len(dataset) / world_size)
    print(f"[Rank {rank}] Assigned ~{total_local} samples for caching.")
    print(f"[Rank {rank}] CUDA ready: {torch.cuda.is_available()}, device={torch.cuda.current_device()}")

    # ---------------------------
    # ìºì‹± ë£¨í”„
    # ---------------------------
    if hasattr(model, "eval"):
        model.eval()

    total_cached, total_skipped, total_processed = 0, 0, 0
    pbar = tqdm(
        total=total_local,
        desc=f"[Rank {rank}] Caching progress",
        dynamic_ncols=True,
        disable=(rank != 0)
    )

    with torch.inference_mode():
        for batch_idx, batch in enumerate(data_loader):
            texts = batch["instruction"]
            image_paths_list = batch["images"]
            keys = batch["cache_keys"]

            # --- ë¯¸ìŠ¤/ìŠ¤í‚µ ë¶„ë¦¬ ---
            miss_items = []
            for key, txt, views in zip(keys, texts, image_paths_list):
                cpath = _cache_path_for(key, txt, views)
                if not cpath.exists():
                    miss_items.append({"text": txt, "views": views, "key": key, "cpath": cpath})
                else:
                    total_skipped += 1

            total_processed += len(keys)
            if not miss_items:
                pbar.update(len(keys))
                if rank == 0:
                    cached_ratio = (total_cached / max(1, total_processed)) * 100
                    pbar.set_postfix({
                        "cached": total_cached,
                        "skipped": total_skipped,
                        "miss%": f"{100 - cached_ratio:.1f}%",
                        "GPU": f"{torch.cuda.memory_allocated(device)/1e9:.1f}GB"
                    })
                continue

            # --- ë©”ì‹œì§€ ì „ì²˜ë¦¬ (CPU) ---
            messages_list = []
            for item in miss_items:
                txt, views = item["text"], item["views"]
                msg_content = [{"type": "image", "image": v} for v in views if v is not None]
                msg_content.append({"type": "text", "text": txt})
                messages_list.append([{"role": "user", "content": msg_content}])

            processed_texts, vision_inputs_list = [], []
            for messages in messages_list:
                text = model.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                vision_inputs, _ = process_vision_info(messages)
                processed_texts.append(text)
                vision_inputs_list.append(vision_inputs)

            # --- ë§ˆì´í¬ë¡œë°°ì¹­ + OOM ë°±ì˜¤í”„ ---
            start = 0
            _micro_bs = max(1, micro_bs)
            while start < len(miss_items):
                end = min(start + _micro_bs, len(miss_items))
                sub_items  = miss_items[start:end]
                sub_texts  = processed_texts[start:end]
                sub_vision = vision_inputs_list[start:end]

                try:
                    inputs = model.processor(
                        text=sub_texts,
                        images=sub_vision,
                        padding=True,
                        return_tensors="pt"
                    ).to(device=device, dtype=torch.bfloat16, non_blocking=True)

                    outputs = model.vl_model(
                        **inputs,
                        output_hidden_states=True,
                        use_cache=False,          # âœ… ë©”ëª¨ë¦¬ ì ˆê°
                        return_dict=True
                    )
                    vl_tokens_batch = outputs.hidden_states[-1]
                    pooled_batch = vl_tokens_batch.mean(dim=1, keepdim=True)

                    for j, item in enumerate(sub_items):
                        pooled_single = pooled_batch[j:j+1]
                        _local_atomic_save(
                            pooled_single.detach().to("cpu", dtype=torch.float16),
                            item["cpath"]
                        )
                        total_cached += 1

                    # ì •ë¦¬
                    del inputs, outputs, vl_tokens_batch, pooled_batch
                    torch.cuda.empty_cache()

                    start = end  # ë‹¤ìŒ ë§ˆì´í¬ë¡œ ë°°ì¹˜ë¡œ ì§„í–‰

                except RuntimeError as e:
                    if "CUDA out of memory" in str(e):
                        torch.cuda.empty_cache()
                        if _micro_bs == 1:
                            raise  # ë” ì¤„ì¼ ìˆ˜ ì—†ìŒ
                        _micro_bs = max(1, _micro_bs // 2)
                        if rank == 0:
                            print(f"[OOM] Lowering micro_bs to #{_micro_bs} and retrying...")
                        continue
                    else:
                        raise

            # --- ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ---
            pbar.update(len(keys))
            if rank == 0:
                cached_ratio = (total_cached / max(1, total_processed)) * 100
                pbar.set_postfix({
                    "cached": total_cached,
                    "skipped": total_skipped,
                    "miss%": f"{100 - cached_ratio:.1f}%",
                    "GPU": f"{torch.cuda.memory_allocated(device)/1e9:.1f}GB"
                })

            # (ì„ íƒ) ì£¼ê¸°ì  ìºì‹œ ìš©ëŸ‰ ì œí•œ
            _local_enforce_cache_limit(max_gb=40)

    pbar.close()
    print(f"[Rank {rank}] âœ… Finished. Cached {total_cached} / Skipped {total_skipped}")
    dist.barrier()
    if rank == 0:
        print("ğŸš€ All ranks finished caching. Cache is ready for training.")
