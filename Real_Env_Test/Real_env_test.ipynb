{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b4e63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/zed_box/miniconda3/envs/qwen_env/lib/python3.10/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/zed_box/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/lib/libtorch_python.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/zed_box/NAS/Qwen2.5-VL-3B-_OCT_FPI_Action_Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/__init__.py:290\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    289\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/zed_box/miniconda3/envs/qwen_env/lib/python3.10/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/zed_box/miniconda3/envs/qwen_env/lib/python3.10/site-packages/torch/lib/libtorch_python.so)"
     ]
    }
   ],
   "source": [
    "# !pip install xformers\n",
    "# !pip install pillow\n",
    "# !pip install transformers==4.57.0\n",
    "# !pip install --no-deps --force-resintall torch # jetson orin AGX version https://developer.download.nvidia.com/compute/redist/jp/\n",
    "# !pip install --no-cache $TORCH_INSTALL torchvision # jetson orin AGX version\n",
    "# !pip install --force-reinstall \"numpy<2.0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/zed_box/NAS/Qwen2.5-VL-3B-_OCT_FPI_Action_Model\")\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import QwenVLAForAction\n",
    "from Total_Dataset import BridgeRawSequenceDataset, collate_fn\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CKPT_PATH = \"../checkpoints/qwen_vla_final_1000.pt\"\n",
    "VL_MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "model = QwenVLAForAction(\n",
    "    vl_model_name=VL_MODEL_NAME,\n",
    "    action_dim=7,\n",
    "    horizon=8,\n",
    "    hidden_dim=1024,\n",
    "    cache_dir=\"./cache/qwen_vl_features\"\n",
    ").to(DEVICE)\n",
    "\n",
    "model.set_cache(False)  # 캐시 기능 자체는 비활성\n",
    "\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=DEVICE, weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Processor (Qwen2.5-VL과 동일한 전처리)\n",
    "processor = AutoProcessor.from_pretrained(VL_MODEL_NAME)\n",
    "\n",
    "# ===========================================\n",
    "# 2️⃣ 예시 입력\n",
    "# ===========================================\n",
    "# 샘플 이미지 로드 (예: 테스트용 jpg)\n",
    "image_path = \"/home/zed_box/NAS/VLA_make_the_dataset/dataset/part2/ZED_Captures_11th/view1/zed_41182735_left_1759125181.869.jpg\"  # 아무 이미지나 하나 두세요\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "instruction = \"Move the gripper towards the white block.\"\n",
    "\n",
    "z_chunk = torch.zeros((1, 8, 7), dtype=torch.bfloat16, device=\"cuda\")\n",
    "\n",
    "# ===========================================\n",
    "# 3️⃣ 모델 입력 구성\n",
    "# ===========================================\n",
    "inputs = processor(\n",
    "    text=instruction,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        text_inputs=instruction,\n",
    "        image_inputs=[[image]],\n",
    "        z_chunk=z_chunk\n",
    "    )\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(f\"✅ Inference time: {elapsed*1000:.2f} ms\")\n",
    "\n",
    "# ===========================================\n",
    "# 5️⃣ 결과 확인\n",
    "# ===========================================\n",
    "if isinstance(output, (tuple, list)):\n",
    "    pred_action = output[0]\n",
    "else:\n",
    "    pred_action = output\n",
    "\n",
    "print(\"Predicted action:\", pred_action)\n",
    "print(\"Shape:\", pred_action.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
