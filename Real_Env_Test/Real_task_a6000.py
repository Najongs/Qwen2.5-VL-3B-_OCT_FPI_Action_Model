# ===========================================
# Real_task_final_jetson_opt.py (Jetson ìµœì í™” + ì•ˆì •ì„± ê°•í™”)
# ===========================================
# pip install xformers
# pip install pillow
# pip install --upgrade transformers

import sys
import time
import numpy as np
import torch
from PIL import Image
import matplotlib.pyplot as plt
from transformers import AutoProcessor

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

sys.path.append("/home/najo/NAS/VLA/Qwen2.5-VL-3B-_OCT_FPI_Action_Model")

from model import QwenVLAForAction
from Total_Dataset import BridgeRawSequenceDataset, collate_fn  # (í˜¸í™˜ì„± ìœ ì§€ìš©)

# ===========================================
# 1ï¸âƒ£ ê¸°ë³¸ ì„¤ì •
# ===========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CKPT_PATH = "../checkpoints/qwen_vla_final_1000.pt"
VL_MODEL_NAME = "Qwen/Qwen2.5-VL-3B-Instruct"

torch.backends.cuda.matmul.allow_tf32 = True  # Ampere ì´ìƒ TF32 í—ˆìš©
torch.backends.cudnn.benchmark = True         # ì…ë ¥ í¬ê¸° ê³ ì • ì‹œ Convolution ìµœì í™”
torch.set_float32_matmul_precision("high")    # GEMM ê³ ì • ì •ë°€ë„ ìƒí–¥

# ===========================================
# 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ
# ===========================================
print("ğŸš€ Loading model...")
model = QwenVLAForAction(
    vl_model_name=VL_MODEL_NAME,
    action_dim=7,
    horizon=8,
    hidden_dim=1024,
    cache_dir="./cache/qwen_vl_features"
).to(DEVICE)

model.set_cache(False)  # ìºì‹œ ë¹„í™œì„±í™” (ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” ë¹„ì¶”ì²œ)
checkpoint = torch.load(CKPT_PATH, map_location=DEVICE, weights_only=True)
model.load_state_dict(checkpoint["model_state_dict"], strict=False)
model.eval()
print("âœ… Model loaded and frozen.\n")

# ===========================================
# 3ï¸âƒ£ Processor ë° ì…ë ¥ ë°ì´í„° ì¤€ë¹„
# ===========================================
processor = AutoProcessor.from_pretrained(VL_MODEL_NAME)

# í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€
image_path = "/home/najo/NAS/VLA/dataset/part1/ZED_Captures_2th/view1/left/zed_41182735_left_1759119936.930.jpg"
image = Image.open(image_path).convert("RGB")

instruction = "Move the gripper towards the white block."
z_chunk = torch.zeros((1, 8, 7), dtype=torch.bfloat16, device=DEVICE)

# processorëŠ” tokenizer ìš©ë„ë¡œë§Œ ì‚¬ìš© (ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ëª¨ë¸ì— ì „ë‹¬)
_ = processor(text=instruction, images=image, return_tensors="pt").to(DEVICE)

# ===========================================
# 4ï¸âƒ£ Warm-up ë‹¨ê³„ (JIT, CUDA ì»¤ë„ ì´ˆê¸°í™”)
# ===========================================
print("ğŸ”¥ Warming up model (3 iters)...")
torch.cuda.synchronize()
with torch.no_grad():
    for _ in range(3):
        _ = model(
            text_inputs=instruction,
            image_inputs=[[image]],
            z_chunk=z_chunk
        )
torch.cuda.synchronize()
print("âœ… Warm-up complete.\n")

# ===========================================
# 5ï¸âƒ£ ì •ë°€í•œ GPU Inference ì‹œê°„ ì¸¡ì •
# ===========================================
print("âš¡ Measuring precise GPU inference time...")

num_runs = 10
times = []

with torch.no_grad():
    for _ in range(num_runs):
        torch.cuda.synchronize()
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)

        start_event.record()
        _ = model(
            text_inputs=instruction,
            image_inputs=[[image]],
            z_chunk=z_chunk
        )
        end_event.record()

        torch.cuda.synchronize()
        elapsed_ms = start_event.elapsed_time(end_event)
        times.append(elapsed_ms)

mean_time = np.mean(times)
std_time = np.std(times)
print(f"âœ… Mean Inference Time: {mean_time:.2f} ms Â± {std_time:.2f} ms\n")

# ===========================================
# 6ï¸âƒ£ ë‹¨ì¼ ì¶”ë¡  ê²°ê³¼ í™•ì¸
# ===========================================
print("ğŸ¯ Running final single inference...")

with torch.no_grad():
    output = model(
        text_inputs=instruction,
        image_inputs=[[image]],
        z_chunk=z_chunk
    )

if isinstance(output, (tuple, list)):
    pred_action = output[0]
else:
    pred_action = output

print("Predicted action:", pred_action)
print("Shape:", pred_action.shape)
print("âœ… Completed successfully.")
