{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VL Î™®Îç∏\n",
    "# PaliGemma2 PT ÏïÑÎãàÎ©¥ MixÎ°ú ÏãúÎèÑÌï¥Î≥¥Í∏∞ // multi image ÏßÄÏõê ÏïàÎê® - Ï†úÏô∏\n",
    "# DeepSeek‚ÄëVL2 tiny 3B\n",
    "# Qwen2.5-VL 3B (AWQÎäî ÏïÑÎãò)\n",
    "\n",
    "# Ï∂îÍ∞ÄÏ†ÅÏù∏ Î™®Îç∏\n",
    "# SAIL-VL2 3B\n",
    "# SmolVLM 3B\n",
    "\n",
    "## OCT/FPI ÏÑºÏÑú Ïù∏ÏΩîÎçî\n",
    "# MLP block?``\n",
    "\n",
    "## Action Experts\n",
    "# \n",
    "\n",
    "## Dataset\n",
    "# BridgeDataV2 - scriptedÎäî Ìä∏Î†àÏ†ùÌÜ†Î¶¨Î•º Ï∞çÏùÄ Îç∞Ïù¥ÌÑ∞ÏÖã = ÎÇ¥Í∫ºÏôÄ Ïú†ÏÇ¨Ìï® -> v2Îßå ÏÇ¨Ïö©ÌïòÎ©∞, Í∑∏Ï§ëÏóêÏÑúÎèÑ datacol2Î•º ÏÇ¨Ïö©\n",
    "# DROID\n",
    "# ALOHA\n",
    "\n",
    "## chunking size\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 4Ô∏è‚É£ Dataset Test & Verification\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # ‚ö†Ô∏è ÏÇ¨Ïö©ÏûêÏùò Îç∞Ïù¥ÌÑ∞ÏÖã Î£®Ìä∏ Í≤ΩÎ°úÎ°ú ÏàòÏ†ïÌï¥Ï£ºÏÑ∏Ïöî.\n",
    "    DATASET_ROOT = \"/home/najo/NAS/VLA/dataset/raw/bridge_data_v2\" \n",
    "    \n",
    "    print(\"üß™ [Step 1] Initializing Dataset...\")\n",
    "    try:\n",
    "        dataset = BridgeRawSequenceDataset(\n",
    "            root=DATASET_ROOT,\n",
    "            horizon=8,\n",
    "            im_size=224,\n",
    "            max_traj=10 # Îπ†Î•∏ ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ trajectory Í∞úÏàò Ï†úÌïú\n",
    "        )\n",
    "        print(f\"‚úÖ Dataset initialized successfully. Found {len(dataset)} samples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Dataset initialization failed: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\nüß™ [Step 2 & 3] Fetching and Verifying a single sample...\")\n",
    "    # --- Îã®Ïùº ÏÉòÌîå ÌÖåÏä§Ìä∏ ---\n",
    "    sample = dataset[0]\n",
    "    images = sample['images']\n",
    "    actions = sample['actions']\n",
    "    instruction = sample['instruction']\n",
    "\n",
    "    print(f\"  - Instruction: '{instruction}'\")\n",
    "    print(f\"  - Images shape: {images.shape}\")\n",
    "    print(f\"     (H, V, C, H, W) = ({images.shape[0]}, {images.shape[1]}, {images.shape[2]}, {images.shape[3]}, {images.shape[4]})\")\n",
    "    print(f\"  - Actions shape: {actions.shape}\")\n",
    "    print(f\"     (H, A) = ({actions.shape[0]}, {actions.shape[1]})\")\n",
    "    print(f\"  - Images dtype: {images.dtype}\")\n",
    "    print(f\"  - Actions dtype: {actions.dtype}\")\n",
    "    print(f\"  - Images value range: [{images.min():.2f}, {images.max():.2f}]\")\n",
    "    print(\"‚úÖ Single sample verification complete.\")\n",
    "\n",
    "\n",
    "    print(\"\\nüß™ [Step 4] Testing with DataLoader...\")\n",
    "    # --- Îç∞Ïù¥ÌÑ∞Î°úÎçî ÌÖåÏä§Ìä∏ ---\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=2 # Î≥ëÎ†¨ Ï≤òÎ¶¨ ÌÖåÏä§Ìä∏\n",
    "    )\n",
    "\n",
    "    batch_sample = next(iter(data_loader))\n",
    "    batch_images = batch_sample['images']\n",
    "    batch_actions = batch_sample['actions']\n",
    "    batch_instructions = batch_sample['instruction']\n",
    "    \n",
    "    print(f\"  - Batched Images shape: {batch_images.shape}\")\n",
    "    print(f\"     (B, H, V, C, H, W)\")\n",
    "    print(f\"  - Batched Actions shape: {batch_actions.shape}\")\n",
    "    print(f\"     (B, H, A)\")\n",
    "    print(f\"  - Batched Instructions: {len(batch_instructions)} instructions\")\n",
    "    print(f\"    e.g., '{batch_instructions[0]}'\")\n",
    "    print(\"‚úÖ DataLoader test successful.\")\n",
    "\n",
    "\n",
    "    print(\"\\nüñºÔ∏è [Bonus] Visualizing a sample image...\")\n",
    "    # --- ÏãúÍ∞ÅÌôî ÌÖåÏä§Ìä∏ ---\n",
    "    # Ï≤´ Î≤àÏß∏ ÏãúÍ∞Ñ Ïä§ÌÖù, Ï≤´ Î≤àÏß∏ Î∑∞Ïùò Ïù¥ÎØ∏ÏßÄÎ•º Í∞ÄÏ†∏Ïò¥\n",
    "    img_tensor = images[0, 0, :, :, :] # (C, H, W)\n",
    "    \n",
    "    # [-1, 1] Î≤îÏúÑÎ•º [0, 1] Î≤îÏúÑÎ°ú ÎêòÎèåÎ¶¨Í∏∞\n",
    "    img_tensor = (img_tensor * 0.5) + 0.5 \n",
    "    \n",
    "    # (C, H, W) -> (H, W, C)Î°ú Ï∂ï ÏàúÏÑú Î≥ÄÍ≤Ω\n",
    "    img_to_show = img_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.imshow(img_to_show)\n",
    "    plt.title(\"Sample Image (t=0, view=0)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8ffe6",
   "metadata": {},
   "source": [
    "## Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1997ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BridgeRawSequenceDataset(Dataset):\n",
    "    def __init__(self, root, horizon=8, im_size=None, num_views=None, transform=None, max_traj=None):\n",
    "        self.root = root\n",
    "        self.horizon = horizon\n",
    "        self.im_size = im_size\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "        # ‚úÖ traj Ìè¥Îçî ÏàòÏßë\n",
    "        self.traj_paths = []\n",
    "        datacols = glob.glob(os.path.join(root, \"datacol2_*\"))\n",
    "        for dc in datacols:\n",
    "            self.traj_paths += glob.glob(os.path.join(dc, \"**\", \"traj*\"), recursive=True)\n",
    "        self.traj_paths = [p for p in self.traj_paths if os.path.exists(os.path.join(p, \"policy_out.pkl\"))]\n",
    "\n",
    "        if max_traj:\n",
    "            self.traj_paths = self.traj_paths[:max_traj]\n",
    "\n",
    "        # ‚úÖ view Í∞úÏàò ÏûêÎèô Í∞êÏßÄ\n",
    "        if num_views is None:\n",
    "            sample_traj = self.traj_paths[0]\n",
    "            view_dirs = sorted([d for d in os.listdir(sample_traj) if d.startswith(\"images\")])\n",
    "            self.num_views = len(view_dirs)\n",
    "        else:\n",
    "            self.num_views = num_views\n",
    "\n",
    "        # ‚úÖ trajectory indexing\n",
    "        self.samples = self._index_chunks()\n",
    "\n",
    "    def _index_chunks(self):\n",
    "        samples = []\n",
    "        for traj_path in tqdm(self.traj_paths, desc=\"Indexing Chunks\"):\n",
    "            # Í∞Å trajÎßàÎã§ Ïù¥ÎØ∏ÏßÄ ÏãúÌÄÄÏä§ Í∏∏Ïù¥ ÌôïÏù∏\n",
    "            img_dir = os.path.join(traj_path, \"images0\")\n",
    "            imgs = sorted(glob.glob(os.path.join(img_dir, \"im_*.jpg\")))\n",
    "            if not imgs:\n",
    "                continue\n",
    "            T = len(imgs)\n",
    "            chunk_count = max(T - self.horizon + 1, 0)\n",
    "            for i in range(chunk_count):\n",
    "                samples.append((traj_path, i))\n",
    "        print(f\"‚úÖ Indexed {len(samples)} chunks from {len(self.traj_paths)} trajectories\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj_path, start_idx = self.samples[idx]\n",
    "\n",
    "        # üîπ Multi-view image sequences\n",
    "        img_seq = []\n",
    "        for t in range(start_idx, start_idx + self.horizon):\n",
    "            views = []\n",
    "            for v in range(self.num_views):\n",
    "                img_path = os.path.join(traj_path, f\"images{v}\", f\"im_{t}.jpg\")\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.im_size:\n",
    "                    img = img.resize((self.im_size, self.im_size))\n",
    "                views.append(self.transform(img))\n",
    "            views = torch.stack(views, dim=0)  # (V, C, H, W)\n",
    "            img_seq.append(views)\n",
    "        img_seq = torch.stack(img_seq, dim=0)  # (H, V, C, H, W)\n",
    "\n",
    "        # üîπ Action sequence\n",
    "        with open(os.path.join(traj_path, \"policy_out.pkl\"), \"rb\") as f:\n",
    "            actions = pickle.load(f)\n",
    "            if isinstance(actions[0], dict):\n",
    "                actions = [a.get(\"actions\") for a in actions if \"actions\" in a]\n",
    "        actions = np.array(actions)\n",
    "        act_seq = torch.tensor(actions[start_idx:start_idx + self.horizon], dtype=torch.float32)\n",
    "\n",
    "        # üîπ Language\n",
    "        lang_path = os.path.join(traj_path, \"lang.txt\")\n",
    "        if os.path.exists(lang_path):\n",
    "            with open(lang_path, \"r\") as f:\n",
    "                lang = f.read().strip()\n",
    "        else:\n",
    "            lang = \"\"\n",
    "\n",
    "        return {\"images\": img_seq, \"actions\": act_seq, \"instruction\": lang}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5838e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 9617.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexed 1299 chunks from 50 trajectories\n",
      "torch.Size([8, 3, 3, 480, 640])\n",
      "torch.Size([8, 7])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/home/najo/NAS/VLA/dataset/raw/bridge_data_v2\"\n",
    "dataset = BridgeRawSequenceDataset(root=root_dir, horizon=8, im_size=None, max_traj=50)\n",
    "sample = dataset[0]\n",
    "\n",
    "print(sample[\"images\"].shape)   # (8, V, 3, H, W)\n",
    "print(sample[\"actions\"].shape)  # (8, A)\n",
    "print(sample[\"instruction\"])    # text or \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a8e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading base model and processor: Qwen/Qwen2.5-VL-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760936e0a2ad4cd5adf6a15236ff2fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßä Freezing the base VL model...\n",
      "‚úÖ VL model has been frozen.\n",
      "==================================================\n",
      "üìä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ Î∂ÑÏÑù\n",
      "==================================================\n",
      "üëÅÔ∏è Vision-Language Î™®Îç∏ (vl_model):\n",
      "  - Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞:     3,754,622,976\n",
      "  - ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: 0 (ÎèôÍ≤∞Îê®)\n",
      "--------------------------------------------------\n",
      "ü§ñ Action Expert Ìó§Îìú (action_expert):\n",
      "  - Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞:     69,302,279\n",
      "  - ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: 69,302,279\n",
      "--------------------------------------------------\n",
      "üìà ÏµúÏ¢Ö Ìï©Í≥Ñ:\n",
      "  - Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞:     3,823,925,255\n",
      "  - ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: 69,302,279 (1.81%)\n",
      "==================================================\n",
      "trainable params: 69302279 || all params: 3823925255 || trainable%: 1.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najo/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted actions: torch.Size([1, 8, 7])\n",
      "Œîactions: torch.Size([1, 8, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# ================================\n",
    "# 1Ô∏è‚É£ Action Expert Head\n",
    "# ================================\n",
    "class QwenActionExpert(nn.Module):\n",
    "    def __init__(self, vl_dim=3072, action_dim=7, horizon=8, hidden_dim=1024, nhead=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.cond_proj = nn.Linear(vl_dim, hidden_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, horizon, hidden_dim))\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim, nhead=nhead, dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.temporal_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, vl_tokens: torch.Tensor, z_chunk: torch.Tensor):\n",
    "        B, H, A = z_chunk.shape\n",
    "\n",
    "        cond = self.cond_proj(vl_tokens.mean(dim=1, keepdim=True))\n",
    "        tgt = self.pos_embed.repeat(B, 1, 1)\n",
    "\n",
    "        decoded = self.temporal_decoder(tgt, cond)  # (B,H,Hd)\n",
    "        delta = self.output_head(decoded)           # (B,H,A)\n",
    "        pred_actions = z_chunk + delta\n",
    "\n",
    "        return pred_actions, delta\n",
    "\n",
    "# ================================\n",
    "# 2Ô∏è‚É£ Full Qwen-VL + ActionExpert Model\n",
    "# ================================\n",
    "class QwenVLAForAction(nn.Module):\n",
    "    def __init__(self, vl_model_name=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "                 action_dim=7, horizon=8, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        print(f\"üöÄ Loading base model and processor: {vl_model_name}\")\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(vl_model_name)\n",
    "        self.vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            vl_model_name,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda\",\n",
    "        )\n",
    "        self.action_expert = QwenActionExpert(\n",
    "            vl_dim=self.vl_model.config.hidden_size,\n",
    "            action_dim=action_dim,\n",
    "            horizon=horizon,\n",
    "            hidden_dim=hidden_dim,\n",
    "        ).to(dtype=torch.bfloat16, device=\"cuda\")\n",
    "        \n",
    "        print(\"üßä Freezing the base VL model...\")\n",
    "        for param in self.vl_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"‚úÖ VL model has been frozen.\")\n",
    "\n",
    "\n",
    "    def forward(self, text_inputs, image_inputs, z_chunk):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img} for img in imgs] + [{\"type\": \"text\", \"text\": txt}]}\n",
    "            for imgs, txt in zip(image_inputs, text_inputs)\n",
    "        ]\n",
    "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        img_inputs, vid_inputs = process_vision_info(messages)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=img_inputs,\n",
    "            videos=vid_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(next(self.parameters()).device)\n",
    "\n",
    "        outputs = self.vl_model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        vl_tokens = outputs.hidden_states[-1]  # (B, T, Dv), dtype=bf16\n",
    "        z_chunk = z_chunk.to(device=vl_tokens.device, dtype=vl_tokens.dtype)\n",
    "        pred_actions, delta = self.action_expert(vl_tokens, z_chunk)\n",
    "\n",
    "        return pred_actions, delta\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Î™®Îç∏Ïùò ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞ ÏàòÎ•º Ï∂úÎ†•Ìï©ÎãàÎã§.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "# ================================\n",
    "# 3Ô∏è‚É£ Example Usage\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    import PIL.Image as Image\n",
    "\n",
    "    # ÏòàÏãú Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏\n",
    "    image_paths = [\n",
    "        \"/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view1/right/zed_41182735_right_1759395176.397.jpg\",\n",
    "        \"/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view2/left/zed_49429257_left_1759395176.393.jpg\",\n",
    "        \"/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view3/left/zed_44377151_left_1759395176.448.jpg\",\n",
    "        \"/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view4/left/zed_49045152_left_1759395176.244.jpg\",\n",
    "        \"/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view5_oak/oak_1944301011169A4800_1759395176.364.jpg\",\n",
    "        \"/home/najo/NAS/VLA/dataset/output.png\"\n",
    "    ]\n",
    "\n",
    "    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî ÌõÑ ÌôïÏù∏\n",
    "    model = QwenVLAForAction(vl_model_name=\"Qwen/Qwen2.5-VL-3B-Instruct\", action_dim=7, horizon=8).to(\"cuda\")\n",
    "    # ================================\n",
    "    # üìä ÌååÎùºÎØ∏ÌÑ∞ ÏÉÅÏÑ∏ Î∂ÑÏÑù\n",
    "    # ================================\n",
    "\n",
    "    # 1. Vision-Language Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞\n",
    "    vl_model_params = sum(p.numel() for p in model.vl_model.parameters())\n",
    "    vl_model_trainable = sum(p.numel() for p in model.vl_model.parameters() if p.requires_grad)\n",
    "\n",
    "    # 2. Action Expert Ìó§Îìú ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞\n",
    "    action_expert_params = sum(p.numel() for p in model.action_expert.parameters())\n",
    "    action_expert_trainable = sum(p.numel() for p in model.action_expert.parameters() if p.requires_grad)\n",
    "\n",
    "    # 3. Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞\n",
    "    total_params = vl_model_params + action_expert_params\n",
    "    trainable_params = vl_model_trainable + action_expert_trainable\n",
    "\n",
    "    # Í≤∞Í≥º Ï∂úÎ†•\n",
    "    print(\"=\"*50)\n",
    "    print(\"üìä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ Î∂ÑÏÑù\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(f\"üëÅÔ∏è Vision-Language Î™®Îç∏ (vl_model):\")\n",
    "    print(f\"  - Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞:     {vl_model_params:,}\")\n",
    "    print(f\"  - ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: {vl_model_trainable:,} (ÎèôÍ≤∞Îê®)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(f\"ü§ñ Action Expert Ìó§Îìú (action_expert):\")\n",
    "    print(f\"  - Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞:     {action_expert_params:,}\")\n",
    "    print(f\"  - ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: {action_expert_trainable:,}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print(\"üìà ÏµúÏ¢Ö Ìï©Í≥Ñ:\")\n",
    "    print(f\"  - Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞:     {total_params:,}\")\n",
    "    print(f\"  - ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    B, H, A = 1, 8, 7\n",
    "    z_chunk = torch.randn(B, H, A).to(\"cuda\")\n",
    "\n",
    "    # forward Ïã§Ìñâ\n",
    "    with torch.no_grad():\n",
    "        pred_actions, delta = model(\n",
    "            text_inputs=[\"Describe the meaning...\"],\n",
    "            image_inputs=[image_paths],\n",
    "            z_chunk=z_chunk\n",
    "        )\n",
    "\n",
    "    print(\"Predicted actions:\", pred_actions.shape)\n",
    "    print(\"Œîactions:\", delta.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e14194",
   "metadata": {},
   "source": [
    "## Flow Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChunkedFlowMatchingExpert(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vl_token_dim: int,\n",
    "        action_dim: int,\n",
    "        chunk_len: int = 5,\n",
    "        hidden_dim: int = 1024,\n",
    "        num_layers: int = 4,\n",
    "        nhead: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.chunk_len = chunk_len\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(action_dim + 1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_proj = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, vl_tokens: torch.Tensor, z_chunk: torch.Tensor, tau: torch.Tensor):\n",
    "        \"\"\"\n",
    "        vl_tokens: (B, T, D)\n",
    "        z_chunk: (B, C, A)\n",
    "        tau: (B, 1) or (B, 1, 1)\n",
    "        \"\"\"\n",
    "        # === [1] Shape Í≤ÄÏÇ¨ Î∞è Ï†ïÎ¶¨ ===\n",
    "        if z_chunk.ndim != 3:\n",
    "            raise ValueError(f\"Expected z_chunk shape (B, C, A), got {z_chunk.shape}\")\n",
    "\n",
    "        if tau.ndim == 2:\n",
    "            tau = tau.unsqueeze(-1)  # (B, 1, 1)\n",
    "        elif tau.ndim != 3:\n",
    "            raise ValueError(f\"tau shape must be (B, 1) or (B, 1, 1), got {tau.shape}\")\n",
    "\n",
    "        B, C, A = z_chunk.shape\n",
    "\n",
    "        # === [2] œÑÎ•º Í∞Å stepÏóê Î≥µÏ†ú ===\n",
    "        tau_expanded = tau.expand(B, C, 1)  # (B, C, 1)\n",
    "\n",
    "        # === [3] ÏûÖÎ†• Í≤∞Ìï© ÌõÑ Ïù∏ÏΩîÎî© ===\n",
    "        z_input = torch.cat([z_chunk, tau_expanded], dim=-1)  # (B, C, A+1)\n",
    "        z_embed = self.input_proj(z_input)  # (B, C, H)\n",
    "\n",
    "        # === [4] Cross-Attention ===\n",
    "        context = self.decoder(tgt=z_embed, memory=vl_tokens)  # (B, C, H)\n",
    "\n",
    "        # === [5] Œî ÏòàÏ∏° Î∞è residualÎ°ú Î≥µÏõê ===\n",
    "        delta = self.output_proj(context)  # (B, C, A)\n",
    "        pred_actions = z_chunk + delta     # residual prediction\n",
    "\n",
    "        return pred_actions, delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61272ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.262145519256592\n"
     ]
    }
   ],
   "source": [
    "# ÏòàÏ†ú Ï∞®Ïõê\n",
    "B, C, A = 4, 5, 6\n",
    "T, D = 64, 1024\n",
    "\n",
    "vl_tokens = torch.randn(B, T, D)\n",
    "x = torch.randn(B, C, A)\n",
    "y = torch.randn(B, C, A)\n",
    "tau = torch.rand(B, 1)\n",
    "\n",
    "# [Ï§ëÏöî] Ï∞®Ïõê ÎßûÏ∂∞Ï£ºÍ∏∞\n",
    "tau = tau[:, None]  # (B, 1, 1) or just let model fix it\n",
    "\n",
    "# Flow Matching noisy input ÏÉùÏÑ±\n",
    "z = tau * x + (1 - tau) * y  # (B, C, A)\n",
    "\n",
    "# Î™®Îç∏ ÏÑ†Ïñ∏ Î∞è Ïã§Ìñâ\n",
    "model = ChunkedFlowMatchingExpert(\n",
    "    vl_token_dim=D,\n",
    "    action_dim=A,\n",
    "    chunk_len=C,\n",
    ")\n",
    "pred, delta = model(vl_tokens, z, tau)\n",
    "\n",
    "# ÏÜêÏã§ Í≥ÑÏÇ∞\n",
    "loss = nn.MSELoss()(delta, y - x)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ccf53",
   "metadata": {},
   "source": [
    "## SAIL-VL2 env test // Ïâ¨ÏõåÎ≥¥ÏûÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model_path = \"your model path\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "device = torch.cuda.current_device()\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16,).to(device)\n",
    "\n",
    "print(\"##### with images\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": 'image_path'}, \n",
    "    {\"type\": \"text\", \"text\": \"describe the image\"}]}\n",
    "]\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "image_path = 'your image path'\n",
    "image = Image.open(image_path)\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device).to(torch.bfloat16)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response = response.split('<|im_end|>')[0].strip()\n",
    "print(response)\n",
    "\n",
    "\n",
    "print(\"##### without images\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"‰∏≠ÂõΩÁöÑÈ¶ñÈÉΩÊòØÂì™ÈáåÔºü\"}]\n",
    "    }\n",
    "]\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = processor(images=None, text=text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device).to(torch.bfloat16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response = response.split('<|im_end|>')[0].strip()\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a8d0d",
   "metadata": {},
   "source": [
    "## Qwen2.5-VL env test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c417ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers accelerate\n",
    "# !pip install transformers==4.57.0\n",
    "# # It's highly recommanded to use `[decord]` feature for faster video loading.\n",
    "# !pip install qwen-vl-utils[decord]==0.0.8\n",
    "# !pip install torchvision>=0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fdb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b455f2e87354e4293c313813d5058c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "image_path1 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view1/right/zed_41182735_right_1759395176.397.jpg'\n",
    "image_path2 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view2/left/zed_49429257_left_1759395176.393.jpg'\n",
    "image_path3 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view3/left/zed_44377151_left_1759395176.448.jpg'\n",
    "image_path4 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view4/left/zed_49045152_left_1759395176.244.jpg'\n",
    "image_path5 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view5_oak/oak_1944301011169A4800_1759395176.364.jpg'\n",
    "oct_path = '/home/najo/NAS/VLA/dataset/output.png'\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path1},\n",
    "            {\"type\": \"image\", \"image\": image_path2},\n",
    "            {\"type\": \"image\", \"image\": image_path3},\n",
    "            {\"type\": \"image\", \"image\": image_path4},\n",
    "            {\"type\": \"image\", \"image\": image_path5},\n",
    "            {\"type\": \"image\", \"image\": oct_path},\n",
    "            # {\"type\": \"text\", \"text\": \"Describe what the robot is doing in detail and explain the end tool needle tip in the image.\"},\n",
    "            # {\"type\": \"text\", \"text\": \"what have to do next? for insertion the needle tip\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe the meaning of the oct image with other images1 and 2\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbec35b",
   "metadata": {},
   "source": [
    "## Deepseek-VL2 env Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc984eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/deepseek-ai/DeepSeek-VL2.git\n",
    "# !pip install -e .\n",
    "# !pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617bf6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.0.1+cu117\n",
      "Torchvision Version: 0.15.2+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Torchvision Version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n",
    "from deepseek_vl2.utils.io import load_pil_images\n",
    "\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/deepseek-vl2-tiny\"\n",
    "vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4957dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najo/.conda/envs/deepseek_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add pad token = ['<ÔΩú‚ñÅpad‚ñÅÔΩú>'] to the tokenizer\n",
      "<ÔΩú‚ñÅpad‚ñÅÔΩú>:2\n",
      "Add image token = ['<image>'] to the tokenizer\n",
      "<image>:128815\n",
      "Add grounding-related tokens = ['<|ref|>', '<|/ref|>', '<|det|>', '<|/det|>', '<|grounding|>'] to the tokenizer with input_ids\n",
      "<|ref|>:128816\n",
      "<|/ref|>:128817\n",
      "<|det|>:128818\n",
      "<|/det|>:128819\n",
      "<|grounding|>:128820\n",
      "Add chat tokens = ['<|User|>', '<|Assistant|>'] to the tokenizer with input_ids\n",
      "<|User|>:128821\n",
      "<|Assistant|>:128822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|User|>: This is image_1: <image>\n",
      "This is image_2: <image>\n",
      "This is image_3: <image>\n",
      "This is image_4: <image>\n",
      "This is image_5: <image>\n",
      " Putting all the photos together describe what the robot is doing in detail and explain the end tool needle tip in the image.\n",
      "\n",
      "<|Assistant|>: The robot in the images appears to be performing tasks related to material handling or processing within an industrial setting. The first image shows a robotic arm positioned over a workbench with various objects placed around it. This setup suggests that the robot may be involved in tasks such as assembly, packaging, or material sorting.\n",
      "\n",
      "In the second image, we see another view of the same workspace, but this time focusing more closely on the robot's interaction with one of the objects on the table. The robot seems to be using its end tool, which looks like a needle tip, to manipulate or interact with the object. This could involve tasks such as picking up, placing, or sorting items.\n",
      "\n",
      "The third image provides a different perspective of the workspace, highlighting the robot's position relative to the workbench and the objects on it. The fourth image shows a close-up of the robot's end tool, which is likely designed for precision tasks. The needle tip is positioned over a small object, indicating that the robot might be in the process of picking up or placing the object onto the workbench.\n",
      "\n",
      "Overall, the images depict a robotic system engaged in material handling or processing activities within an industrial environment. The end tool needle tip plays a crucial role in these tasks, enabling the robot to interact with and manipulate objects accurately and efficiently.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    }
   ],
   "source": [
    "image_path1 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view1/right/zed_41182735_right_1759395176.397.jpg'\n",
    "image_path2 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view2/left/zed_49429257_left_1759395176.393.jpg'\n",
    "image_path3 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view3/left/zed_44377151_left_1759395176.448.jpg'\n",
    "image_path4 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view4/left/zed_49045152_left_1759395176.244.jpg'\n",
    "image_path5 = '/home/najo/NAS/VLA/dataset/OCT_insertion/Captures4/view5_oak/oak_1944301011169A4800_1759395176.364.jpg'\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"This is image_1: <image>\\n\"\n",
    "                   \"This is image_2: <image>\\n\"\n",
    "                   \"This is image_3: <image>\\n\"\n",
    "                   \"This is image_4: <image>\\n\"\n",
    "                   \"This is image_5: <image>\\n Putting all the photos together describe what the robot is doing in detail and explain the end tool needle tip in the image.\",\n",
    "        \"images\": [\n",
    "            image_path1,\n",
    "            image_path2,\n",
    "            image_path3,\n",
    "            image_path4,\n",
    "            image_path5,\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "# load images and prepare for inputs\n",
    "pil_images = load_pil_images(conversation)\n",
    "prepare_inputs = vl_chat_processor(\n",
    "    conversations=conversation,\n",
    "    images=pil_images,\n",
    "    force_batchify=True,\n",
    "    system_prompt=\"\"\n",
    ").to(vl_gpt.device)\n",
    "\n",
    "# run image encoder to get the image embeddings\n",
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "# run the model to get the response\n",
    "outputs = vl_gpt.language.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=prepare_inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=False)\n",
    "print(f\"{prepare_inputs['sft_format'][0]}\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3211bd",
   "metadata": {},
   "source": [
    "## Paligemma 2 env test // ÌóàÍπÖÌéòÏù¥Ïä§ Î°úÍ∑∏Ïù∏ ÏïàÎê® ÎÇòÏ§ëÏóê ÏãúÎèÑÌïòÏûê - multi image input ÏïàÎê®;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40889aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/paligemma-3b-mix-224.\n401 Client Error. (Request ID: Root=1-68ea79dd-4d6de4e101f2c6916d6e30ca;1e98ae80-8d1b-4148-9f7d-5ef2570a0cdf)\n\nCannot access gated repo for url https://huggingface.co/google/paligemma-3b-mix-224/resolve/bfloat16/config.json.\nAccess to model google/paligemma-3b-mix-224 is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:407\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/paligemma-3b-mix-224/resolve/bfloat16/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1117\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1658\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1655\u001b[0m ):\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1546\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1463\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:424\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    421\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     )\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-68ea79dd-4d6de4e101f2c6916d6e30ca;1e98ae80-8d1b-4148-9f7d-5ef2570a0cdf)\n\nCannot access gated repo for url https://huggingface.co/google/paligemma-3b-mix-224/resolve/bfloat16/config.json.\nAccess to model google/paligemma-3b-mix-224 is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPaliGemmaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     19\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Instruct the model to create a caption in Spanish\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/modeling_utils.py:4846\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   4845\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 4846\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4855\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4857\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4858\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4859\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4860\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   4862\u001b[0m         model_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf_file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/configuration_utils.py:622\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 622\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    624\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key]\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/configuration_utils.py:662\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/configuration_utils.py:721\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/utils/hub.py:543\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/paligemma-3b-mix-224.\n401 Client Error. (Request ID: Root=1-68ea79dd-4d6de4e101f2c6916d6e30ca;1e98ae80-8d1b-4148-9f7d-5ef2570a0cdf)\n\nCannot access gated repo for url https://huggingface.co/google/paligemma-3b-mix-224/resolve/bfloat16/config.json.\nAccess to model google/paligemma-3b-mix-224 is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/paligemma-3b-mix-224\"\n",
    "device = \"cuda:0\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device,\n",
    "    revision=\"bfloat16\",\n",
    ").eval()\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Instruct the model to create a caption in Spanish\n",
    "prompt = \"caption es\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2db84a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea34f8fbe0e3431193cf6da74bec20bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = load_image(url)\n",
    "\n",
    "feature_extractor = pipeline(\n",
    "    model=\"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\",\n",
    "    task=\"image-feature-extraction\", \n",
    ")\n",
    "features = feature_extractor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb524bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najo/.conda/envs/dinov3/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8661712f72e14562b3f1a7dfa2370f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user wants a short introduction to large language models. Let me start by defining what they are. Large language models are AI systems trained on vast amounts of text data. I should mention their purpose, like generating text, understanding language, and being used in various applications.\n",
      "\n",
      "I need to keep it concise. Maybe start with a definition, then talk about their training, the types like GPT, and their applications. Also, mention the challenges they face, like bias and data quality. But since it's a short intro, maybe just touch on the key points without going too deep. Make sure to highlight their ability to generate human-like text and their impact on different fields. Avoid technical jargon to keep it accessible. Check for any errors and ensure the flow is logical.\n",
      "</think>\n",
      "content: Large language models (LLMs) are advanced AI systems designed to understand, generate, and interact with human language. Trained on vast amounts of text data, they excel at tasks like writing essays, coding, or answering questions. These models use deep learning techniques to learn patterns in language, enabling them to produce coherent, context-aware responses. They are widely used in applications such as content creation, customer service, and scientific research. However, their effectiveness depends on data quality and the complexity of the tasks they perform.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83501b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276f021e6f704d0d9208eb619c9a71e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1513ed071fee44849740da8425039966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vision Model Output Dim: 768\n",
      "‚úÖ Language Model Input Dim: 2048\n",
      "\n",
      "--- Î™®Îç∏Ïùò ÎãµÎ≥Ä ---\n",
      ", more comprehensive, more comprehensive, more comprehensive, more comprehensive, more comprehensive, more comprehensive, more comprehensive, more comprehensive, more comprehensive, more comprehensive, more comprehensive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# -- 1. Î™®Îç∏ Î∞è ÌîÑÎ°úÏÑ∏ÏÑú Î°úÎìú --\n",
    "\n",
    "# Vision Model (DINOv3)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\")\n",
    "vision_model = AutoModel.from_pretrained(\"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\")\n",
    "\n",
    "# Language Model (Qwen3)\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "vision_model.to(llm.device)\n",
    "\n",
    "# -- 2. Ïù¥ÎØ∏ÏßÄ ÌäπÏßïÏùÑ Ïñ∏Ïñ¥ Î™®Îç∏Ïùò ÏûÖÎ†• Í≥µÍ∞ÑÏúºÎ°ú Î≥ÄÌôòÌïòÎäî ÌîÑÎ°úÏ†ùÏÖò Î†àÏù¥Ïñ¥ Ï†ïÏùò --\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy_inputs = torch.randn(1, 3, 224, 224).to(llm.device)\n",
    "    dummy_outputs = vision_model(dummy_inputs)\n",
    "    vision_hidden_dim = dummy_outputs.last_hidden_state.shape[-1] \n",
    "\n",
    "llm_hidden_dim = llm.config.hidden_size\n",
    "print(f\"‚úÖ Vision Model Output Dim: {vision_hidden_dim}\")\n",
    "print(f\"‚úÖ Language Model Input Dim: {llm_hidden_dim}\")\n",
    "\n",
    "projection_layer = nn.Linear(vision_hidden_dim, llm_hidden_dim).to(llm.device, dtype=llm.dtype)\n",
    "\n",
    "# -- 3. Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ Î∞è Ï≤òÎ¶¨ --\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "prompt = \"Ïù¥ Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï§ò.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    pixel_values = image_processor(images=image, return_tensors=\"pt\").pixel_values.to(llm.device, dtype=vision_model.dtype)\n",
    "    image_features = vision_model(pixel_values).last_hidden_state\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "text_inputs = tokenizer(text_template, return_tensors=\"pt\").to(llm.device)\n",
    "\n",
    "# -- 4. Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© Í≤∞Ìï© --\n",
    "\n",
    "text_embeddings = llm.get_input_embeddings()(text_inputs.input_ids)\n",
    "\n",
    "# === Ïó¨Í∏∞Îßå ÏàòÏ†ïÎêòÏóàÏäµÎãàÎã§! ===\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌäπÏßïÏùò Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏùÑ LLMÍ≥º ÎèôÏùºÌïòÍ≤å ÎßûÏ∂∞Ï§çÎãàÎã§.\n",
    "image_features_casted = image_features.to(llm.dtype)\n",
    "# ÌÉÄÏûÖÏù¥ ÎßûÏ∂∞ÏßÑ ÌäπÏßïÏùÑ ÌîÑÎ°úÏ†ùÏÖò Î†àÏù¥Ïñ¥Ïóê ÌÜµÍ≥ºÏãúÌÇµÎãàÎã§.\n",
    "image_embeddings = projection_layer(image_features_casted)\n",
    "# ===========================\n",
    "\n",
    "combined_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
    "image_attention_mask = torch.ones(image_embeddings.shape[:2], dtype=torch.long, device=llm.device)\n",
    "combined_attention_mask = torch.cat([image_attention_mask, text_inputs.attention_mask], dim=1)\n",
    "\n",
    "# -- 5. LLMÏùÑ ÌÜµÌï¥ ÎãµÎ≥Ä ÏÉùÏÑ± --\n",
    "\n",
    "generated_ids = llm.generate(\n",
    "    inputs_embeds=combined_embeddings,\n",
    "    attention_mask=combined_attention_mask,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "output_ids = generated_ids[0][combined_embeddings.shape[1]:].tolist() \n",
    "response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Î™®Îç∏Ïùò ÎãµÎ≥Ä ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ff18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
